Trieuthehien123
Pass: Trieuthehien@1721050400

- x√°c ƒë·ªãnh s·ªë k trong thu·∫≠t to√°n knn c√≥ th·ªÉ x√°c ƒë·ªãnh b·∫±ng c√°ch n·∫øu s·ªë ph√¢n l·ªõp l√† ch·∫µn th√¨ k ch·ªçn s·∫Ω l√† c√°c s·ªë l·∫ª
ho·∫∑c s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p elbow ƒë·ªÉ x√°c ƒë·ªãnh k.

- Anh em M√¨ qu·∫•t ngay n√®! Freeeeeeeeee!
[L∆∞u ngay] 14 K√äNH ONLINE T·ª∞ H·ªåC MI·ªÑN PH√ç MACHINE LEARNING  
ü§ñ H·ªçc m√°y (machine learning) l√† m·ªôt nh√°nh (m·∫£ng con) c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o cung c·∫•p cho c√°c h·ªá th·ªëng v√† m√°y m√≥c kh·∫£ nƒÉng t·ª± h·ªçc v√† t·ªëi ∆∞u h√≥a c√°c quy tr√¨nh m√† kh√¥ng c·∫ßn c√°c nh√† ph√°t tri·ªÉn (developers) ph·∫£i l·∫≠p tr√¨nh r√µ r√†ng.
ü§ñ L∆∞u ngay 14 website v√† k√™nh youtube gi√∫p b·∫°n t·ª± h·ªçc #MachineLearning nh√©!
üëâ Machine Learning by Andrew NG - https://www.coursera.org/learn/machine-learning
üëâ Intro to ML by Udacity - https://www.udacity.com/.../intro-to-machine-learning--ud120
üëâ EdX‚Äôs Learning from Data(Introductory Machine Learning) - https://www.edx.org/.../learning-from-data-introductory...#!
üëâ Introduction to Machine Learning for Coders - http://course18.fast.ai/ml
üëâ Statistical Machine Learning by CMU - https://www.youtube.com/watch...
üëâ Coursera‚Äôs Neural Networks for Machine Learning - https://www.youtube.com/watch...
üëâ Kaggle Complete Roadmap for Machine Learning - https://www.kaggle.com/learn/overview
üëâ EdX‚Äôs Principles of Machine Learning - https://www.edx.org/course/principles-of-machine-learning
üëâ Coursera‚Äôs Machine Learning Specialization - https://www.coursera.org/specializations/machine-learning
üëâ Machine Learning Crash Course by Google - https://developers.google.com/machine-learning/crash-course
üëâ Machine Learning Course at W3Schools - https://www.w3schools.com/.../python_ml_getting_started.asp
üëâ Intro to Machine Learning Course at Kaggle - https://www.kaggle.com/learn/intro-to-machine-learning
üëâ Intermediate Machine Learning Course at Kaggle - https://www.kaggle.com/learn/intermediate-machine-learning
üëâ Machine Learning with Python - https://cognitiveclass.ai/.../machine-learning-with-python



===========================================


- MATPLOTLIB
	+ plt.legend(loc="lower right") #ch·ªânh v·ªã tr√≠ c·ªßa ch√∫ th√≠ch ·ªü ƒë√¢y l√† g√≥c ph·∫£i b√™n d∆∞·ªõi
	+ plt.xticks(rotation=50) #ch·ªânh g√≥c nghi√™ng c·ªßa label tr√™n tr·ª•c x

- SQL
	+ COUNT(*) ƒë·∫øm c·∫£ NULL c√≤n COUNT(colunm) th√¨ kh√¥ng
	--------------
	+ C√°c b∆∞·ªõc chu·∫©n h√≥a CSDL:
		1NF: M·ªói √¥ c·ªßa b·∫£ng ch·ªâ n√™n c√≥ duy nh·∫•t 1 gi√° tr·ªã, M·ªói b·∫£n ghi s·∫Ω l√† duy nh·∫•t
		2NF: Tr∆∞·ªõc ti√™n ph·∫£i tu√¢n th·ªß nguy√™n t·∫Øc 1NF,  Kh√≥a ch√≠nh l√† m·ªôt c·ªôt ƒë∆°n(ri√™ng l·∫ª).
		3NF: ƒë·∫£m b·∫£o nguy√™n t·∫Øc c·ªßa 2NF, Kh√¥ng c√≥ s·ª± b·∫Øc c·∫ßu trong ph·ª• thu·ªôc h√†m gi·ªØa c√°c thu·ªôc t√≠nh.
	+ CEIL(): l√†m tr√≤n l√™n
	+ PIVOT(): chuy·ªÉn b·∫£ng d·∫°ng c·ªôt th√†nh h√†ng
	+ DATEDIFF(): t√≠nh s·ª± ch√™nh l·ªách gi·ªØa ng√†y, th√°ng, nƒÉm, ....

- BIG DATA
	+ Hadoop d√πng ƒë·ªÉ l∆∞u tr·ªØ v√† x·ª≠ l√Ω d·ªØ li·ªáu
	+ C·∫•u tr√∫c c·ªßa HDFS g·ªìm: NameNode(master) v√† DataNode
	+ Hadoop Map reduce d√πng ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu ·ªü Hadoop version1 sau sang version2 ƒë∆∞·ª£c thay th·∫ø b·∫±ng spark
	+ DataWarehouse ƒë∆∞·ª£c l·∫•y t·ª´ DB d√πng ƒë·ªÉ ph·ª•c cho report v√† c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng nhi·ªÅu l·∫ßn
	+ ETL (Extract - Transform - Load)
	+ 2 m√¥ h√¨nh thi·∫øt k·∫ø DataWarehouse ph·ªï bi·∫øn ƒë√≥ l√†:  Star Schema v√† Snowflake
	+ Datalake l√† m·ªôt kho l∆∞u tr·ªØ trung t√¢m ch·ª©a Raw Data (d·ªØ li·ªáu th√¥). 
	D·ªØ li·ªáu b√™n trong Datalake l√† d·ªØ li·ªáu ho√†n to√†n ch∆∞a qua x·ª≠ l√Ω.
	+ NOTE: m√¥ h√¨nh th√¨ quy tr√¨nh ETL ph·∫£i t·ª´ Database ‚Üí Datalake ‚Üí Data Warehouse.
	+ DataMart l√† m√¥ h√¨nh thu nh·ªè c·ªßa DataWarehouse ho·∫∑c l√† m·ªôt DataWarehose ƒë∆∞·ª£c chia nh·ªè th√†nh nhi·ªÅu ph·∫ßn
	+Data Warehouse lu√¥n ƒë∆∞·ª£c thi·∫øt k·∫ø ·ªü d·∫°ng OLAP cho m·ª•c ƒë√≠ch Analytics (read l√† ch√≠nh, c√≤n insert, update, delete l√† ph·ª•) 
	c√≤n OLTP d√†nh cho RDBMS thi√™n v·ªÅ vi·ªác update record

	-------------------------------------------------------------------------------------------

	Hadoop cmd
	+ Hadoop version: ki·ªÉm tra phi√™n b·∫£n hadoop
	+ Hadoop star-all.cmd: Kh·ªüi ƒë·ªông t√¢t c·∫£ h·ªá th·ªëng hadoop
	+ Hadoop start/stop-dfs.cmd: kh·ªüi ƒë·ªông ho·∫∑c t·∫Øt name node
	+ Hadoop start/stop-yarn.cmd: kh·ªüi ƒë·ªông ho·∫∑c t·∫Øt data node
	+ Hadoop jps:
	+ check list c√°c file trong folder: hdfs dfs -ls <ten duong dan> 
	+ T·∫°o folder m·ªõi: hdfs dfs -mkdir [-P] /mydata
	+ put data from local machine to HDFS:
		hdfs dfs -put <source> <dest> => ex: hdfs dfs -put /C:/Downloads/... /mydata 
		hdfs dfs -coppyFromLocal <source> <dest>
		hdfs dfs -moveFromLocal <source> <dest>
	+ get data from HDFS to localmachine
		hdfs dfs -get <source> <dest> => ex: hdfs dfs -get /mydata  /C:/Downloads/...
		hdfs dfs coppyToLocal <source> <dest>
		hdfs dfs -moveToLocal <src> <localDest> c√°i n√†y s·∫Ω x√≥a file ho·∫∑c th∆∞ m·ª•c trong hdfs r·ªìi di chuy·ªÉn
	+ read content file
		hdfs dfs -cat <ten file> => ex: hdfs dfs -cat /mydata/code.txt
		hdfs dfs -tail ten file>
	+ check dung l∆∞·ª£ng ƒë√£ s·ª≠ d·ª•ng tr√™n hdfs
		hdfs dfs -du -s -h <ten folder> (k√≠ch th∆∞·ªõc t·ªïng c·ªßa folder)
		hdfs dfs -du -h <ten folder> (k√≠ch th∆∞·ªõc c·ªßa t·ª´ng file trrong folder)
	+ check dung l∆∞·ª£ng free
		hdfs dfs -df -h <ten folder>
	+ thay ƒë·ªïi quy·ªÅn truy c·∫≠p user
		hdfs dfs -chmod 755 /mydata/code.txt
	+ ki·ªÉm tra th·ªùi gian s·ª≠a ƒë·ªïi file g·∫ßn nh·∫•t
		hdfs dfs -stat /mydata/code.txt
	+ di chuy·ªÉn t·ªáp ho·∫∑c th∆∞ m·ª•c ƒë·∫øn ch·ªó kh√°c
		hdfs dfs -mv <src><dest>
	+ coppy t·ªáp ho·∫∑c th∆∞ m·ª•c ƒë·∫øn ch·ªó kh√°c
		hdfs dfs -cp <src> <dest>
	+ x√≥a file ho·∫∑c th∆∞ m·ª•c
		hdfs dfs -rm <path>
	+ Truy xu·∫•t t·∫•t c·∫£ c√°c t·ªáp ph√π h·ª£p v·ªõi ƒë∆∞·ªùng d·∫´n src trong HDFS v√† sao ch√©p ch√∫ng v√†o m·ªôt t·ªáp duy nh·∫•t
		hdfs dfs -getmerge <src> <localDest>
	-------------------------------------------------

	HADOOP THEORY
	
	introduce hadoop
	- What does SQOOP stand for? sql to hadoop
	- What is considered to be part of the Apache Basic Hadoop Modules? HDFS
	- What are the two major components of the MapReduce layer? TaskManager, JobTracker
	- What does HDFS stand for? Hdoop distributed file system
	- What are the two majority types of nodes in HDFS? Datanode and Namenode
	- What is Yarn used as an alternative to in Hadoop 2.0 and higher versions of Hadoop? Mapreduce
	- Could you run an existing MapReduce application using Yarn? Yes
	- What are the two basic layers comprising the Hadoop Architecture? MapReduce and HDFS
	- What are Hadoop advantages over a traditional platform? Scalability, Reliability, Flexibility, Cost
	**********

	- Choose features introduced in Hadoop2 HDFS?
		Heterogenous storage including SSD RAM_DISK
		Multiple namespaces 
		HDFS Federation
	- In Hadoop2 HDFS a namespace can generate block IDs for new blocks without coordinating with other namespaces. True
	- This is a new feature in YARN:
		High Availability ResourceManager,
		web services REST APIs
		ApplicationMasters
	- Apache Tez can run independent of YARN: False
	- In Hadoop2 with YARN
		Each application has its own ApplicationMaster

	************
	Hadoop excution enviroment
	- Apache Spark cannot operate without YARN? False
	- Apache Tez can support dynamic DAG changes? True
	- Give an example of an execution framework that supports cyclic data flow? Spark
	- The Fairshare scheduler can support queues/sub-queues? True
	- The Capacity Scheduler can use ACLs to control security? True
	- Mark choices that apply for Apache Spark:
		Can run integrated with YARN
		Supports in memory computing
		Can be accessed/used from high level languages like Java, Scala, Python, and R.
	- Which of the following choices apply for Apache Tez?
		Supports complex directed acyclic graph (DAG) of tasks
		Supports in memory caching of data
		Improves resource usage efficiency
	*********
	Hadoop application
	- Applications that can run within Hadoop: HBase, Cassandra
	- Name the high level language that is a main part of Apache Pig: PigLatin
	- Apache Pig can only be run using scripts: False
	- Check options that are methods of using/accessing Hive: Hcatalog, Beeline, WebHcat
	- Check features that apply for HBase
		Non-relational distributed database	
		Compression
	- List methods of accessing HBase:
		Apache HBase shell
		HBase External API
		HBase API
	**********
	HDFS achitecture
	- HDFS is strictly POSIX compliant: False
	- Following issues may be caused by lot of small files in HDFS
		NameNode memory usage increases significantly
		Network load decreases
		Number of map tasks need to process the same amount of data will be larger.
	- You are writing a 10GB file into HDFS with a replication of 
	2 and block size of 64MB. How much total disk space will this file use? 20GB
	- What is the first step in a write process from a HDFS client?
		Immediately contact the NameNode

	- HDFS NameNode is not rack aware when it places the replica blocks: False
	***********
	HDFS perfomance, tuning, and robustness(Hi·ªáu su·∫•t HDFS, ƒëi·ªÅu ch·ªânh v√† ƒë·ªô m·∫°nh m·∫Ω)
	- Name the configuration file which holds HDFS tuning parameters
		hdfs-site.xml
	- Name the parameter that controls the replication factor in HDFS:
		dfs.replication
	- Check answers that apply when replication is lowered
		HDFS is less robust
		less likely make data local to more workers
		more space
	- Check answers that apply when NameNode fails to receive heartbeat from a DataNode
		DataNode is marked dead
		No new I/O is sent to particular DataNode that missed heartbeat check
		Blocks below replication factor are re-replicated on other DataNodes
	- How is data corruption mitigated in HDFS
		checksums are computed on file creation and stored in HDFS namespace for verification when data is retrieved.
	*************
	accessing HDFS
	- Which of the following are valid access mechanisms for HDFS
		Accessed via Java API,
		Accessed via HTTP
	- Which of the following commands will give information on the status of DataNodes
		hdfs dfsadmin -report
	**************
	MapReduce
	- Which of these kinds of data motivated the Map/Reduce framework?
		Large number of internet documents that need to be indexed for searching by words
	- What is the organizing data structure for map/reduce programs?
		A list of identification keys and some value associated with that identifier
	- In map/reduce framework, which of these logistics does Map/Reduce do with the map function?
		Distribute map to cluster nodes, run map on the data partitions at the same time
	- Map/Reduce performs a ‚Äòshuffle‚Äô and grouping. That means it...
		Shuffles <key,value> pairs into different partitions according to the key value, and sorts within the partitions by key.
	- In the word count example, what is the key?
		The word itself.
	- Streaming map/reduce allows mappers and reducers to be written in what languages:
		All of the above
	- The assignment asked you to run with 2 reducers. When you use 2 reducers instead of 1 reducer, what is the difference in global sort order?
		With 1 reducer, but not 2 reducers, the word counts are in global sort order by word.
	***************
	Spark1
	- Apache Spark was developed in order to provide solutions to shortcomings of another project, and eventually replace it. What is the name of this project?
		MapReduce
	- Why is Hadoop MapReduce slow for iterative algorithms?
		It needs to read off disk for every iteration
	- What is the most important feature of Apache Spark to speedup iterative algorithms?
		Caching datasets in memory
	- Which other Hadoop project can Spark rely to provision and manage the cluster of nodes?
		YARN
	- When Spark reads data out of HDFS, what is the process that interfaces directly with HDFS?
		Executor
	- Under which circumstances is preferable to run Spark in Standalone mode instead of relying on YARN?
		When you only plan on running Spark jobs

	*************
	Spark2
	- .How can you create an RDD? Mark all that apply
		Reading from a local file available both on the driver and on the workers
		Reading from HDFS
		Apply a transformation to an existing RDD
	- How does Spark make RDDs resilient in case a partition is lost?
		Tracks the history of each partition and reruns what is needed to restore it
	- Which of the following sentences about flatMap and map are true?
		flatMap accepts a function that returns multiple elements, those elements are then flattened out into a continuous RDD.
		map transforms elements with a 1 to 1 relationship, 1 input - 1 output
	- Check all wide transformations
		Repartition, even if it triggers a shuffle, 
		can improve performance of your pipeline by balancing the data distribution after a heavy filtering operation
	*************
	Spark3
	- Check all true statements about the Directed Acyclic Graph Scheduler
		The DAG is managed by the cluster manager
		A DAG is used to track dependencies of each partition of each RDD
	- Why is building a DAG necessary in Spark but not in MapReduce?
		Because MapReduce always has the same type of workflow, Spark needs to accommodate diverse workflows.
	- What are the differences between an action and a transformation? Mark all that apply
		A transformation is from worker nodes to worker nodes, an action between worker nodes and the Driver (or a data source like HDFS)
		A transformation is lazy, an action instead executes immediately.
	- Generally, which are good stages to mark a RDD for caching in memory?
		The first RDD, just after reading from disk, so we avoid reading from disk again.
		At the start of an iterative algorithm.
	- What are good cases for using a broadcast variable? Mark all that apply
		Copy a small/medium sized RDD for a join
		Copy a large lookup table to all worker nodes
		Copy a large configuration dictionary to all worker nodes
	-