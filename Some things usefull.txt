Trieuthehien123
Pass: Trieuthehien@1721050400

- xÃ¡c Ä‘á»‹nh sá»‘ k trong thuáº­t toÃ¡n knn cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh báº±ng cÃ¡ch náº¿u sá»‘ phÃ¢n lá»›p lÃ  cháºµn thÃ¬ k chá»n sáº½ lÃ  cÃ¡c sá»‘ láº»
hoáº·c sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p elbow Ä‘á»ƒ xÃ¡c Ä‘á»‹nh k.

- Anh em MÃ¬ quáº¥t ngay nÃ¨! Freeeeeeeeee!
[LÆ°u ngay] 14 KÃŠNH ONLINE Tá»° Há»ŒC MIá»„N PHÃ MACHINE LEARNING  
ğŸ¤– Há»c mÃ¡y (machine learning) lÃ  má»™t nhÃ¡nh (máº£ng con) cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o cung cáº¥p cho cÃ¡c há»‡ thá»‘ng vÃ  mÃ¡y mÃ³c kháº£ nÄƒng tá»± há»c vÃ  tá»‘i Æ°u hÃ³a cÃ¡c quy trÃ¬nh mÃ  khÃ´ng cáº§n cÃ¡c nhÃ  phÃ¡t triá»ƒn (developers) pháº£i láº­p trÃ¬nh rÃµ rÃ ng.
ğŸ¤– LÆ°u ngay 14 website vÃ  kÃªnh youtube giÃºp báº¡n tá»± há»c #MachineLearning nhÃ©!
ğŸ‘‰ Machine Learning by Andrew NG - https://www.coursera.org/learn/machine-learning
ğŸ‘‰ Intro to ML by Udacity - https://www.udacity.com/.../intro-to-machine-learning--ud120
ğŸ‘‰ EdXâ€™s Learning from Data(Introductory Machine Learning) - https://www.edx.org/.../learning-from-data-introductory...#!
ğŸ‘‰ Introduction to Machine Learning for Coders - http://course18.fast.ai/ml
ğŸ‘‰ Statistical Machine Learning by CMU - https://www.youtube.com/watch...
ğŸ‘‰ Courseraâ€™s Neural Networks for Machine Learning - https://www.youtube.com/watch...
ğŸ‘‰ Kaggle Complete Roadmap for Machine Learning - https://www.kaggle.com/learn/overview
ğŸ‘‰ EdXâ€™s Principles of Machine Learning - https://www.edx.org/course/principles-of-machine-learning
ğŸ‘‰ Courseraâ€™s Machine Learning Specialization - https://www.coursera.org/specializations/machine-learning
ğŸ‘‰ Machine Learning Crash Course by Google - https://developers.google.com/machine-learning/crash-course
ğŸ‘‰ Machine Learning Course at W3Schools - https://www.w3schools.com/.../python_ml_getting_started.asp
ğŸ‘‰ Intro to Machine Learning Course at Kaggle - https://www.kaggle.com/learn/intro-to-machine-learning
ğŸ‘‰ Intermediate Machine Learning Course at Kaggle - https://www.kaggle.com/learn/intermediate-machine-learning
ğŸ‘‰ Machine Learning with Python - https://cognitiveclass.ai/.../machine-learning-with-python



===========================================


- MATPLOTLIB
	- plt.legend(loc="lower right") #chá»‰nh vá»‹ trÃ­ cá»§a chÃº thÃ­ch á»Ÿ Ä‘Ã¢y lÃ  gÃ³c pháº£i bÃªn dÆ°á»›i
	- plt.xticks(rotation=50) #chá»‰nh gÃ³c nghiÃªng cá»§a label trÃªn trá»¥c x
	- tranpose(): hoÃ¡n Ä‘á»•i hÃ ng vá» cá»™t
	- line chart: muá»‘n plot Ä‘á»‘i tÆ°á»£ng thÃ¬ dá»¯ liá»‡u cá»§a Ä‘á»‘i tÆ°á»£ng pháº£i theo cá»™t trong dataframe hoáº·c Ä‘Æ°a vá» dáº¡ng series
- PANDAS
	- dÃ¹ng pivot_table(index='column', columns='column', values='column',...) Ä‘á»ƒ tá»•ng há»£p 
	- nÃ³ dÃ¹ng chia 1 cá»™t thÃ nh nhiá»u cá»™t khÃ¡c Ä‘Æ°á»£c nhÃ³m cÃ¹ng má»™t nhÃ³m
	- TÆ°Æ¡ng tá»± cÃ³ thá»ƒ dá»¥ng groupBy() + count() + unstack()
	- hoáº·c pd.crosstab(index, column) hoáº·c melt()
	- 

- SQL
	+ COUNT(*) Ä‘áº¿m cáº£ NULL cÃ²n COUNT(colunm) thÃ¬ khÃ´ng
	--------------
	+ CÃ¡c bÆ°á»›c chuáº©n hÃ³a CSDL:
		1NF: Má»—i Ã´ cá»§a báº£ng chá»‰ nÃªn cÃ³ duy nháº¥t 1 giÃ¡ trá»‹, Má»—i báº£n ghi sáº½ lÃ  duy nháº¥t
		2NF: TrÆ°á»›c tiÃªn pháº£i tuÃ¢n thá»§ nguyÃªn táº¯c 1NF,  KhÃ³a chÃ­nh lÃ  má»™t cá»™t Ä‘Æ¡n(riÃªng láº»).
		3NF: Ä‘áº£m báº£o nguyÃªn táº¯c cá»§a 2NF, KhÃ´ng cÃ³ sá»± báº¯c cáº§u trong phá»¥ thuá»™c hÃ m giá»¯a cÃ¡c thuá»™c tÃ­nh.
	+ CEIL(): lÃ m trÃ²n lÃªn
	+ PIVOT(): chuyá»ƒn báº£ng dáº¡ng cá»™t thÃ nh hÃ ng
	+ DATEDIFF(): tÃ­nh sá»± chÃªnh lá»‡ch giá»¯a ngÃ y, thÃ¡ng, nÄƒm, ....

- BIG DATA
	+ Hadoop dÃ¹ng Ä‘á»ƒ lÆ°u trá»¯ vÃ  xá»­ lÃ½ dá»¯ liá»‡u
	+ Cáº¥u trÃºc cá»§a HDFS gá»“m: NameNode(master) vÃ  DataNode
	+ Hadoop Map reduce dÃ¹ng Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u á»Ÿ Hadoop version1 sau sang version2 Ä‘Æ°á»£c thay tháº¿ báº±ng spark
	+ DataWarehouse Ä‘Æ°á»£c láº¥y tá»« DB dÃ¹ng Ä‘á»ƒ phá»¥c cho report vÃ  cÃ³ thá»ƒ tÃ¡i sá»­ dá»¥ng nhiá»u láº§n
	+ ETL (Extract - Transform - Load)
	+ 2 mÃ´ hÃ¬nh thiáº¿t káº¿ DataWarehouse phá»• biáº¿n Ä‘Ã³ lÃ :  Star Schema vÃ  Snowflake
	+ Datalake lÃ  má»™t kho lÆ°u trá»¯ trung tÃ¢m chá»©a Raw Data (dá»¯ liá»‡u thÃ´). 
	Dá»¯ liá»‡u bÃªn trong Datalake lÃ  dá»¯ liá»‡u hoÃ n toÃ n chÆ°a qua xá»­ lÃ½.
	+ NOTE: mÃ´ hÃ¬nh thÃ¬ quy trÃ¬nh ETL pháº£i tá»« Database â†’ Datalake â†’ Data Warehouse.
	+ DataMart lÃ  mÃ´ hÃ¬nh thu nhá» cá»§a DataWarehouse hoáº·c lÃ  má»™t DataWarehose Ä‘Æ°á»£c chia nhá» thÃ nh nhiá»u pháº§n
	+Data Warehouse luÃ´n Ä‘Æ°á»£c thiáº¿t káº¿ á»Ÿ dáº¡ng OLAP cho má»¥c Ä‘Ã­ch Analytics (read lÃ  chÃ­nh, cÃ²n insert, update, delete lÃ  phá»¥) 
	cÃ²n OLTP dÃ nh cho RDBMS thiÃªn vá» viá»‡c update record

	-------------------------------------------------------------------------------------------

	Hadoop cmd
	+ Hadoop version: kiá»ƒm tra phiÃªn báº£n hadoop
	+ Hadoop star-all.cmd: Khá»Ÿi Ä‘á»™ng tÃ¢t cáº£ há»‡ thá»‘ng hadoop
	+ Hadoop start/stop-dfs.cmd: khá»Ÿi Ä‘á»™ng hoáº·c táº¯t name node
	+ Hadoop start/stop-yarn.cmd: khá»Ÿi Ä‘á»™ng hoáº·c táº¯t data node
	+ Hadoop jps:
	+ check list cÃ¡c file trong folder: hdfs dfs -ls <ten duong dan> 
	+ Táº¡o folder má»›i: hdfs dfs -mkdir [-P] /mydata
	+ put data from local machine to HDFS:
		hdfs dfs -put <source> <dest> => ex: hdfs dfs -put /C:/Downloads/... /mydata 
		hdfs dfs -coppyFromLocal <source> <dest>
		hdfs dfs -moveFromLocal <source> <dest>
	+ get data from HDFS to localmachine
		hdfs dfs -get <source> <dest> => ex: hdfs dfs -get /mydata  /C:/Downloads/...
		hdfs dfs coppyToLocal <source> <dest>
		hdfs dfs -moveToLocal <src> <localDest> cÃ¡i nÃ y sáº½ xÃ³a file hoáº·c thÆ° má»¥c trong hdfs rá»“i di chuyá»ƒn
	+ read content file
		hdfs dfs -cat <ten file> => ex: hdfs dfs -cat /mydata/code.txt
		hdfs dfs -tail ten file>
	+ check dung lÆ°á»£ng Ä‘Ã£ sá»­ dá»¥ng trÃªn hdfs
		hdfs dfs -du -s -h <ten folder> (kÃ­ch thÆ°á»›c tá»•ng cá»§a folder)
		hdfs dfs -du -h <ten folder> (kÃ­ch thÆ°á»›c cá»§a tá»«ng file trrong folder)
	+ check dung lÆ°á»£ng free
		hdfs dfs -df -h <ten folder>
	+ thay Ä‘á»•i quyá»n truy cáº­p user
		hdfs dfs -chmod 755 /mydata/code.txt
	+ kiá»ƒm tra thá»i gian sá»­a Ä‘á»•i file gáº§n nháº¥t
		hdfs dfs -stat /mydata/code.txt
	+ di chuyá»ƒn tá»‡p hoáº·c thÆ° má»¥c Ä‘áº¿n chá»— khÃ¡c
		hdfs dfs -mv <src><dest>
	+ coppy tá»‡p hoáº·c thÆ° má»¥c Ä‘áº¿n chá»— khÃ¡c
		hdfs dfs -cp <src> <dest>
	+ xÃ³a file hoáº·c thÆ° má»¥c
		hdfs dfs -rm -r <path>
	+ Truy xuáº¥t táº¥t cáº£ cÃ¡c tá»‡p phÃ¹ há»£p vá»›i Ä‘Æ°á»ng dáº«n src trong HDFS vÃ  sao chÃ©p chÃºng vÃ o má»™t tá»‡p duy nháº¥t
		hdfs dfs -getmerge <src> <localDest>
	+ Náº¾U FILE HOáº¶C ÄÆ¯á»œNG DáºªN ÄANG á» CHáº¾ Äá»˜ SAFE MODE MUá»N XÃ“A THÃŒ PHáº¢I Táº®T NÃ“ ÄI Má»šI XÃ“A ÄÆ¯á»¢C
		hdfs dfsadmin -safemode leave
	+ muá»‘n báº­t láº¡i safe mode thÃ¬
		hdfs dfsadmin -safemode enter
	+ kiá»ƒm tra safe mode Ä‘ang á»Ÿ tráº¡ng thÃ¡i nÃ o
		hdfs dfsadmin -safemode get
	-------------------------------------------------

	HADOOP THEORY
	
	introduce hadoop
	- What does SQOOP stand for? sql to hadoop
	- What is considered to be part of the Apache Basic Hadoop Modules? HDFS
	- What are the two major components of the MapReduce layer? TaskManager, JobTracker
	- What does HDFS stand for? Hdoop distributed file system
	- What are the two majority types of nodes in HDFS? Datanode and Namenode
	- What is Yarn used as an alternative to in Hadoop 2.0 and higher versions of Hadoop? Mapreduce
	- Could you run an existing MapReduce application using Yarn? Yes
	- What are the two basic layers comprising the Hadoop Architecture? MapReduce and HDFS
	- What are Hadoop advantages over a traditional platform? Scalability, Reliability, Flexibility, Cost
	**********

	- Choose features introduced in Hadoop2 HDFS?
		Heterogenous storage including SSD RAM_DISK
		Multiple namespaces 
		HDFS Federation
	- In Hadoop2 HDFS a namespace can generate block IDs for new blocks without coordinating with other namespaces. True
	- This is a new feature in YARN:
		High Availability ResourceManager,
		web services REST APIs
		ApplicationMasters
	- Apache Tez can run independent of YARN: False
	- In Hadoop2 with YARN
		Each application has its own ApplicationMaster

	************
	Hadoop excution enviroment
	- Apache Spark cannot operate without YARN? False
	- Apache Tez can support dynamic DAG changes? True
	- Give an example of an execution framework that supports cyclic data flow? Spark
	- The Fairshare scheduler can support queues/sub-queues? True
	- The Capacity Scheduler can use ACLs to control security? True
	- Mark choices that apply for Apache Spark:
		Can run integrated with YARN
		Supports in memory computing
		Can be accessed/used from high level languages like Java, Scala, Python, and R.
	- Which of the following choices apply for Apache Tez?
		Supports complex directed acyclic graph (DAG) of tasks
		Supports in memory caching of data
		Improves resource usage efficiency
	*********
	Hadoop application
	- Applications that can run within Hadoop: HBase, Cassandra
	- Name the high level language that is a main part of Apache Pig: PigLatin
	- Apache Pig can only be run using scripts: False
	- Check options that are methods of using/accessing Hive: Hcatalog, Beeline, WebHcat
	- Check features that apply for HBase
		Non-relational distributed database	
		Compression
	- List methods of accessing HBase:
		Apache HBase shell
		HBase External API
		HBase API

	*******************************************************************************************

	HDFS achitecture
	- HDFS is strictly POSIX compliant: False
	- Following issues may be caused by lot of small files in HDFS
		NameNode memory usage increases significantly
		Network load decreases
		Number of map tasks need to process the same amount of data will be larger.
	- You are writing a 10GB file into HDFS with a replication of 
	2 and block size of 64MB. How much total disk space will this file use? 20GB
	- What is the first step in a write process from a HDFS client?
		Immediately contact the NameNode

	- HDFS NameNode is not rack aware when it places the replica blocks: False

	*******************************************************************************************

	HDFS perfomance, tuning, and robustness(Hiá»‡u suáº¥t HDFS, Ä‘iá»u chá»‰nh vÃ  Ä‘á»™ máº¡nh máº½)
	- Name the configuration file which holds HDFS tuning parameters
		hdfs-site.xml
	- Name the parameter that controls the replication factor in HDFS:
		dfs.replication
	- Check answers that apply when replication is lowered
		HDFS is less robust
		less likely make data local to more workers
		more space
	- Check answers that apply when NameNode fails to receive heartbeat from a DataNode
		DataNode is marked dead
		No new I/O is sent to particular DataNode that missed heartbeat check
		Blocks below replication factor are re-replicated on other DataNodes
	- How is data corruption mitigated in HDFS
		checksums are computed on file creation and stored in HDFS namespace for verification when data is retrieved.

	***************************************************************************************

	accessing HDFS
	- Which of the following are valid access mechanisms for HDFS
		Accessed via Java API,
		Accessed via HTTP
	- Which of the following commands will give information on the status of DataNodes
		hdfs dfsadmin -report

	********************************************************************************

	MapReduce
	- Which of these kinds of data motivated the Map/Reduce framework?
		Large number of internet documents that need to be indexed for searching by words
	- What is the organizing data structure for map/reduce programs?
		A list of identification keys and some value associated with that identifier
	- In map/reduce framework, which of these logistics does Map/Reduce do with the map function?
		Distribute map to cluster nodes, run map on the data partitions at the same time
	- Map/Reduce performs a â€˜shuffleâ€™ and grouping. That means it...
		Shuffles <key,value> pairs into different partitions according to the key value, and sorts within the partitions by key.
	- In the word count example, what is the key?
		The word itself.
	- Streaming map/reduce allows mappers and reducers to be written in what languages:
		All of the above
	- The assignment asked you to run with 2 reducers. When you use 2 reducers instead of 1 reducer, what is the difference in global sort order?
		With 1 reducer, but not 2 reducers, the word counts are in global sort order by word.

	****************************************************************************************

	Spark1
	- Apache Spark was developed in order to provide solutions to shortcomings of another project, and eventually replace it. What is the name of this project?
		MapReduce
	- Why is Hadoop MapReduce slow for iterative algorithms?
		It needs to read off disk for every iteration
	- What is the most important feature of Apache Spark to speedup iterative algorithms?
		Caching datasets in memory
	- Which other Hadoop project can Spark rely to provision and manage the cluster of nodes?
		YARN
	- When Spark reads data out of HDFS, what is the process that interfaces directly with HDFS?
		Executor
	- Under which circumstances is preferable to run Spark in Standalone mode instead of relying on YARN?
		When you only plan on running Spark jobs

	*************
	Spark2
	- .How can you create an RDD? Mark all that apply
		Reading from a local file available both on the driver and on the workers
		Reading from HDFS
		Apply a transformation to an existing RDD
	- How does Spark make RDDs resilient in case a partition is lost?
		Tracks the history of each partition and reruns what is needed to restore it
	- Which of the following sentences about flatMap and map are true?
		flatMap accepts a function that returns multiple elements, those elements are then flattened out into a 			continuous RDD.
		map transforms elements with a 1 to 1 relationship, 1 input - 1 output
	- Check all wide transformations
		Repartition, even if it triggers a shuffle, 
		can improve performance of your pipeline by balancing the data distribution after a heavy filtering 			operation

	************************************************************************

	Spark3
	- Check all true statements about the Directed Acyclic Graph Scheduler
		The DAG is managed by the cluster manager
		A DAG is used to track dependencies of each partition of each RDD
	- Why is building a DAG necessary in Spark but not in MapReduce?
		Because MapReduce always has the same type of workflow, Spark needs to accommodate diverse workflows.
	- What are the differences between an action and a transformation? Mark all that apply
		A transformation is from worker nodes to worker nodes, an action between worker nodes and the Driver (or a 			data source like HDFS)
		A transformation is lazy, an action instead executes immediately.
	- Generally, which are good stages to mark a RDD for caching in memory?
		The first RDD, just after reading from disk, so we avoid reading from disk again.
		At the start of an iterative algorithm.
	- What are good cases for using a broadcast variable? Mark all that apply
		Copy a small/medium sized RDD for a join
		Copy a large lookup table to all worker nodes
		Copy a large configuration dictionary to all worker nodes
	
	===============================================================================================

	- TRONG SPARK RDD cÃ³ 2 kiá»ƒu operator:
		+ Transformation: lazy evalation: Biáº¿n Ä‘á»•i tá»« 1 RDD nÃ y sang 1 hoáº·c nhiá»u RDD khÃ¡c
			*Narow Transformation: dÃ¹ng Ä‘á»ƒ tÃ­nh trong 1 partition vÃ  sá»­ dá»¥ng dá»¯ liá»‡u cá»§a má»™t parttion khÃ¡c
				vd: map, flatmap, Union(gom 2 RDD láº¡i vá»›i nhau), Sample, MapPatition, Filter,..
			*Wide Transformation: sá»­ dá»¥ng dá»¯ liá»‡u tá»« nhiá»u parttion khÃ¡c
				vd: -Intersect(join 2 data vá»›i nhau rá»“i láº¥y pháº§n chung cá»§a 2 data Ä‘Ã³),
				    -Distinct(láº¥y ra pháº§n tá»­ khÃ´ng trÃ¹ng nhau),                                     
				    -ReduceByKey(tÃ­nh toÃ¡n trÃªn tá»«ng cá»¥m rá»“i má»›i tá»•ng há»£p háº¿t láº¡i => 				                                    thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng nhiá»u hÆ¡n), 
				    -GroupByKey(tá»•ng há»£p háº¿t cÃ¡c cá»¥m láº¡i rá»“i má»›i tÃ­nh toÃ¡n  => Ã­t sá»­ dá»¥ng hÆ¡n), 
				    -CombineByKey, 
				    -Join(Káº¿t ná»‘i 2 data vá»›i nhau), 
				    -RePartition(co dÃ£n partition cá»§a dá»¯ liá»‡u tÄƒng hoáº·c giáº£m dá»¯ liá»‡u),
				    -coalesce(co dÃ£n partition cá»§a dá»¯ liá»‡u chá»‰ giáº£m dá»¯ liá»‡u),....
		+ Action: Tráº£ vá» value
			vd: count, collect, take, top, CountByValue,...
	==================================================================================================
	
	- Persist() and cache():
		+ lÃ  ká»¹ thuáº­t Ä‘Æ°á»£c sá»­ dá»¥ng trong tá»‘i Æ°u hiá»‡u nÄƒng tÃ­nh toÃ¡n
		+ thuáº­n lá»£i cho viá»‡c tÃ­nh toÃ¡n song song
		+ cache(): lÆ°u káº¿t quáº£ trung gian vÃ o RAM
		+ persist(): cÃ³ nhiá»u lá»±a chá»n lÆ°u trÃªn RAM hoáº·c trÃªn DISK hoáº·c cáº£ 2
	
	- select() is a transformation that returns a new DataFrame and holds the columns that are selected whereas 				collect() is an action that returns the entire data set in an Array to the driver

	- withColumn() is a transformation function of DataFrame which is used to change the value, convert the datatype of 		an existing column, create a new column, and many more.
	
	- pivot() function to rotate the data from one column into multiple columns. 

	- partitionBy() is a way to split a large dataset into smaller datasets based on one or more partition keys

	- While you are create Data Lake out of Azure, HDFS or AWS you need to understand how to partition your data at 		rest (File system/disk), PySpark partitionBy() and repartition() help you partition the data and 				eliminating the Data Skew on your large datasets.

	- Row_Number(): xáº¿p háº¡ng k cÃ³ duplicate
	- rank(): xáº¿p háº¡ng cÃ³ dupplicate vÃ  thá»© tá»± k liÃªn tiáº¿p
	- dense_rank(): xáº¿p háº¡ng cÃ³ duplicate vÃ  thá»© tá»± liÃªn tiáº¿p

	=============================================================================================================

	ERROR IN PYSPARK
	
	- AttributeError: 'function' object has no attribute '_get_object_id': Ä‘áº·t tÃªn trÃ¹ng vá»›i functions
	- Náº¿u Ä‘á»ƒ show() á»Ÿ ngay sau thÃ¬ kiá»ƒu cá»§a Ä‘Ã´i tÆ°á»£ng sáº½ lÃ  NonType => Ta pháº£i táº¡o biáº¿n vÃ  gá»i show() á»Ÿ biáº¿n Ä‘Ã£ gÃ¡n

	=====================================================================================================

KAFKA
	- zookeeper: Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ quáº£n lÃ½ cÃ¡c broker
	- broker : chá»©a cÃ¡c Partition
	- partition: nÆ¡i lÆ°u trá»¯ cÃ¡c topic
	- topic: nÆ¡i dá»¯ liá»‡u chuyá»n qua giá»¯a producer vÃ  consumer
	- producer: Ä‘á»ƒ publish message vÃ o cÃ¡c topic lÃ  bÃªn gá»­i dá»¯ liá»‡u
	- consumer: Ä‘á»ƒ subscribe vÃ o topic lÃ  bÃªn nháº­n dá»¯ liá»‡u
	- localhost zookeeper 2181
	- locahost kafka 9092

	- CÃ¡c bÆ°á»›c run kafka trÃªn window:
	- b1: cd tá»›i á»• chá»©a kafka => cd c:\kafka\
	- b2: cháº¡y zookeeper 
		.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties
	- b3: cháº¡y kafka
		.\bin\windows\kafka-server-start.bat .\config\server.properties
	- b4: táº¡o topic
		.\bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 		--topic TestTopic
	- b5: check list topic trong zookeeper
		.\bin\windows\kafka-topics.bat --list --zookeeper localhost:2181
	- b6: cháº¡y producer Ä‘á»ƒ táº¡o ra message
		.\bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic TestTopic
	- b7: cháº¡y consumer Ä‘á»ƒ check message Ä‘Ã£ gá»­i tá»›i chÆ°a
		.\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic TestTopic --from-		beginning
		

	
	

 